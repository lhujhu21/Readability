{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom nltk.tokenize import sent_tokenize\nimport nltk\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import auc, roc_curve, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib as plt","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:38:57.24516Z","iopub.execute_input":"2021-08-02T23:38:57.245769Z","iopub.status.idle":"2021-08-02T23:38:58.605176Z","shell.execute_reply.started":"2021-08-02T23:38:57.245634Z","shell.execute_reply":"2021-08-02T23:38:58.604318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:38:58.606556Z","iopub.execute_input":"2021-08-02T23:38:58.606936Z","iopub.status.idle":"2021-08-02T23:38:58.71212Z","shell.execute_reply.started":"2021-08-02T23:38:58.606884Z","shell.execute_reply":"2021-08-02T23:38:58.711305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of words in a text\ntrain['wordCount'] = [len(i) for i in train['excerpt']]\ntest['wordCount'] = [len(i) for i in test['excerpt']]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:38:58.713865Z","iopub.execute_input":"2021-08-02T23:38:58.714242Z","iopub.status.idle":"2021-08-02T23:38:58.728399Z","shell.execute_reply.started":"2021-08-02T23:38:58.714202Z","shell.execute_reply":"2021-08-02T23:38:58.727568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OLD WORD LENGTH\ndef tokenize(texts):\n    vec = CountVectorizer()\n    result = vec.fit_transform(texts)\n    vocab = np.array(vec.get_feature_names())\n    return [vocab[result[i].indices] for i in range(result.shape[0])]\n# average word length of words in a text\ndef wordLengthOld(texts):\n    wordList = tokenize(texts)\n    return [np.mean([len(word) for word in sublist]) for sublist in wordList]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:38:58.730125Z","iopub.execute_input":"2021-08-02T23:38:58.730704Z","iopub.status.idle":"2021-08-02T23:38:58.737556Z","shell.execute_reply.started":"2021-08-02T23:38:58.730667Z","shell.execute_reply":"2021-08-02T23:38:58.736699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NEW WORD LENGTH\ndef wordLength(texts):\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    wordList = [tokenizer.tokenize(txt) for txt in texts]\n    return [np.mean([len(word) for word in sublist]) for sublist in wordList]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:38:58.738795Z","iopub.execute_input":"2021-08-02T23:38:58.739149Z","iopub.status.idle":"2021-08-02T23:38:58.745529Z","shell.execute_reply.started":"2021-08-02T23:38:58.739104Z","shell.execute_reply":"2021-08-02T23:38:58.744843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['wordLength'] = wordLength(train[\"excerpt\"])\ntest['wordLength'] = wordLength(test[\"excerpt\"])\n#train.plot.scatter('target', 'wordLength')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:38:58.746894Z","iopub.execute_input":"2021-08-02T23:38:58.74733Z","iopub.status.idle":"2021-08-02T23:38:59.156149Z","shell.execute_reply.started":"2021-08-02T23:38:58.747294Z","shell.execute_reply":"2021-08-02T23:38:59.155025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SENTENCE LENGTH\ndef sentenceLength(texts):\n    sentenceList = [sent_tokenize(txt) for txt in texts]\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    return [np.mean([len(tokenizer.tokenize(txt)) for txt in sublist]) for sublist in sentenceList]\n\ndef sentenceCount(texts):\n    sentenceList = [sent_tokenize(txt) for txt in texts]\n    return [(len(txt)) for txt in sentenceList]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:38:59.161149Z","iopub.execute_input":"2021-08-02T23:38:59.161662Z","iopub.status.idle":"2021-08-02T23:38:59.168586Z","shell.execute_reply.started":"2021-08-02T23:38:59.161624Z","shell.execute_reply":"2021-08-02T23:38:59.167712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['sentenceLength'] = sentenceLength(train[\"excerpt\"])\ntest['sentenceLength'] = sentenceLength(test[\"excerpt\"])\n#train.plot.scatter('target', 'sentenceLength')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:38:59.171071Z","iopub.execute_input":"2021-08-02T23:38:59.171553Z","iopub.status.idle":"2021-08-02T23:39:00.822453Z","shell.execute_reply.started":"2021-08-02T23:38:59.171516Z","shell.execute_reply":"2021-08-02T23:39:00.821522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['sentenceCount'] = sentenceCount(train[\"excerpt\"])\ntest['sentenceCount'] = sentenceCount(test[\"excerpt\"])\n#train.plot.scatter('target', 'sentenceCount')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:00.824372Z","iopub.execute_input":"2021-08-02T23:39:00.824722Z","iopub.status.idle":"2021-08-02T23:39:02.059717Z","shell.execute_reply.started":"2021-08-02T23:39:00.824687Z","shell.execute_reply":"2021-08-02T23:39:02.058735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def syllables(texts):\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    wordList = [tokenizer.tokenize(txt) for txt in texts]\n    return [np.mean([_syllables(word) for word in sublist]) for sublist in wordList]\n    \ndef _syllables(word):\n    syllable_count = 0\n    vowels = 'aeiouy'\n    if word[0] in vowels:\n        syllable_count += 1\n    for index in range(1, len(word)):\n        if word[index] in vowels and word[index - 1] not in vowels:\n            syllable_count += 1\n    if word.endswith('e'):\n        syllable_count -= 1\n    if word.endswith('le') and len(word) > 2 and word[-3] not in vowels:\n        syllable_count += 1\n    if syllable_count == 0:\n        syllable_count += 1\n    return syllable_count","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:02.061972Z","iopub.execute_input":"2021-08-02T23:39:02.062651Z","iopub.status.idle":"2021-08-02T23:39:02.070928Z","shell.execute_reply.started":"2021-08-02T23:39:02.062605Z","shell.execute_reply":"2021-08-02T23:39:02.070172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['syllables'] = syllables(train[\"excerpt\"])\ntest['syllables'] = syllables(test[\"excerpt\"])\n#train.plot.scatter('target', 'syllables')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:02.073623Z","iopub.execute_input":"2021-08-02T23:39:02.07387Z","iopub.status.idle":"2021-08-02T23:39:03.151195Z","shell.execute_reply.started":"2021-08-02T23:39:02.073845Z","shell.execute_reply":"2021-08-02T23:39:03.150321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stats based method","metadata":{}},{"cell_type":"code","source":"model_lr = LinearRegression()\nmodel_lr.fit(train[[\"wordCount\", \"wordLength\", \"sentenceLength\", \"sentenceCount\", \"syllables\"]],train[\"target\"])\np_lr = model_lr.predict(train[[\"wordCount\", \"wordLength\", \"sentenceLength\", \"sentenceCount\", \"syllables\"]])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:03.152552Z","iopub.execute_input":"2021-08-02T23:39:03.153045Z","iopub.status.idle":"2021-08-02T23:39:03.345908Z","shell.execute_reply.started":"2021-08-02T23:39:03.153006Z","shell.execute_reply":"2021-08-02T23:39:03.344776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_squared_error(train[\"target\"], p_lr)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:03.347506Z","iopub.execute_input":"2021-08-02T23:39:03.348016Z","iopub.status.idle":"2021-08-02T23:39:03.359878Z","shell.execute_reply.started":"2021-08-02T23:39:03.347973Z","shell.execute_reply":"2021-08-02T23:39:03.358861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stats based method with xgboost","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBRegressor","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:03.361399Z","iopub.execute_input":"2021-08-02T23:39:03.361753Z","iopub.status.idle":"2021-08-02T23:39:03.452369Z","shell.execute_reply.started":"2021-08-02T23:39:03.361718Z","shell.execute_reply":"2021-08-02T23:39:03.451377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_stats = train[[\"wordCount\", \"wordLength\", \"sentenceLength\", \"sentenceCount\", \"syllables\"]]\ny = train[\"target\"]\nmodel_stats = XGBRegressor(objective = \"reg:squarederror\", max_depth=10, n_estimators=100)\nmodel_stats.fit(X_stats, y)\np_stats = model_stats.predict(X_stats)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:03.457348Z","iopub.execute_input":"2021-08-02T23:39:03.457711Z","iopub.status.idle":"2021-08-02T23:39:05.417143Z","shell.execute_reply.started":"2021-08-02T23:39:03.457676Z","shell.execute_reply":"2021-08-02T23:39:05.416154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_squared_error(train[\"target\"], p_stats)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:05.418589Z","iopub.execute_input":"2021-08-02T23:39:05.418925Z","iopub.status.idle":"2021-08-02T23:39:05.428028Z","shell.execute_reply.started":"2021-08-02T23:39:05.41889Z","shell.execute_reply":"2021-08-02T23:39:05.426986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ensemble xgboost and logistic regression","metadata":{}},{"cell_type":"code","source":"#p_stats_test = model_stats.predict(test[[\"wordCount\", \"wordLength\", \"sentenceLength\", \"sentenceCount\", \"syllables\"]])\n# p_lr_test = model_lr.predict(test[[\"wordCount\", \"wordLength\", \"sentenceLength\", \"sentenceCount\", \"syllables\"]])\n# test[\"target\"] = .7 * p_lr_test + .3 * p_stats_test\n# test[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:05.42939Z","iopub.execute_input":"2021-08-02T23:39:05.429809Z","iopub.status.idle":"2021-08-02T23:39:05.433117Z","shell.execute_reply.started":"2021-08-02T23:39:05.429775Z","shell.execute_reply":"2021-08-02T23:39:05.432302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Word2Vec with xgboost","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_lg\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:05.434626Z","iopub.execute_input":"2021-08-02T23:39:05.435321Z","iopub.status.idle":"2021-08-02T23:39:13.314235Z","shell.execute_reply.started":"2021-08-02T23:39:05.435286Z","shell.execute_reply":"2021-08-02T23:39:13.310975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = np.array([nlp(text).vector for text in train[\"excerpt\"]])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.315402Z","iopub.status.idle":"2021-08-02T23:39:13.316128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_w2v = embeddings\ny = train[\"target\"]\nmodel_w2v = XGBRegressor(objective = \"reg:squarederror\", max_depth=10, n_estimators=100)\nmodel_w2v.fit(X_w2v, y)\np_w2v = model_w2v.predict(X_w2v)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.317267Z","iopub.status.idle":"2021-08-02T23:39:13.3181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Word2vec with linear regression","metadata":{}},{"cell_type":"code","source":"model_w2v_lr = LinearRegression()\nmodel_w2v_lr.fit(X_w2v,y)\np_w2v_lr = model_w2v_lr.predict(X_w2v)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.319261Z","iopub.status.idle":"2021-08-02T23:39:13.319965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Combination stats and word2vec (linear regression)","metadata":{}},{"cell_type":"code","source":"X_combo = np.concatenate((X_stats, X_w2v), axis = 1)\nmodel_combo_lr = LinearRegression()\nmodel_combo_lr.fit(X_combo, y)\np_combo_lr = model_combo_lr.predict(X_combo)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.320991Z","iopub.status.idle":"2021-08-02T23:39:13.321655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"mean_squared_error(train[\"target\"], p_combo_lr)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T18:20:38.742957Z","iopub.status.idle":"2021-07-31T18:20:38.743495Z"}}},{"cell_type":"markdown","source":"#### Combination stats and word2vec (xgboost)","metadata":{}},{"cell_type":"code","source":"model_combo_xgb = XGBRegressor(objective = \"reg:squarederror\", max_depth=10, n_estimators=100)\nmodel_combo_xgb.fit(X_combo, y)\np_combo_xgb = model_combo_lr.predict(X_combo)\nmean_squared_error(train[\"target\"], p_combo_xgb)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.32269Z","iopub.status.idle":"2021-08-02T23:39:13.323404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"embeddingsTest = np.array([nlp(text).vector for text in test[\"excerpt\"]])\nX_stats_test = test[[\"wordCount\", \"wordLength\", \"sentenceLength\", \"sentenceCount\", \"syllables\"]]\np_combo_lr_test = model_combo_lr.predict(np.concatenate((X_stats_test, embeddingsTest), axis = 1))\np_combo_xgb_test = model_combo_xgb.predict(np.concatenate((X_stats_test, embeddingsTest), axis = 1))\ntest[\"target\"] = 0.7 * p_combo_lr_test + 0.3 * p_combo_xgb_test\ntest[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:52:46.692711Z","iopub.execute_input":"2021-07-31T19:52:46.693106Z","iopub.status.idle":"2021-07-31T19:52:46.98589Z","shell.execute_reply.started":"2021-07-31T19:52:46.693066Z","shell.execute_reply":"2021-07-31T19:52:46.985003Z"}}},{"cell_type":"markdown","source":"#### Nearest neighbor","metadata":{}},{"cell_type":"markdown","source":"from sklearn.neighbors import KNeighborsRegressor\ndef cos(a, b):\n    return np.dot(a, b) / np.sqrt(np.dot(a, a) * np.dot(b, b))\nmodel_nbr = KNeighborsRegressor(n_neighbors=5)#, metric = cos)\nmodel_nbr.fit(embeddings, y)\np_nbr = model_nbr.predict(embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:52:46.987106Z","iopub.execute_input":"2021-07-31T19:52:46.987447Z"}}},{"cell_type":"markdown","source":"mean_squared_error(train[\"target\"], p_nbr)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T18:20:38.749312Z","iopub.status.idle":"2021-07-31T18:20:38.749866Z"}}},{"cell_type":"markdown","source":".751\nembeddingsTest = np.array([nlp(text).vector for text in test[\"excerpt\"]])\ntest[\"target\"] = model_nbr.predict(embeddingsTest)\ntest[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)","metadata":{}},{"cell_type":"markdown","source":"model_combo_nbr = KNeighborsRegressor(n_neighbors=5)#, metric = cos)\nmodel_combo_nbr.fit(np.concatenate((X_stats, embeddings), axis = 1), y)\np_combo_nbr = model_combo_nbr.predict(np.concatenate((X_stats, embeddings), axis = 1))","metadata":{}},{"cell_type":"markdown","source":"mean_squared_error(train[\"target\"], p_combo_nbr)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T18:20:38.753564Z","iopub.status.idle":"2021-07-31T18:20:38.754235Z"}}},{"cell_type":"markdown","source":"embeddingsTest = np.array([nlp(text).vector for text in test[\"excerpt\"]])\nX_stats_test = test[[\"wordCount\", \"wordLength\", \"sentenceLength\", \"sentenceCount\", \"syllables\"]]\np_combo_lr_test = model_combo_lr.predict(np.concatenate((X_stats_test, embeddingsTest), axis = 1))\np_combo_xgb_test = model_combo_xgb.predict(np.concatenate((X_stats_test, embeddingsTest), axis = 1))\np_combo_nbr_test = model_combo_nbr.predict(np.concatenate((X_stats_test, embeddingsTest), axis = 1))\ntest[\"target\"] = 0.6 * p_combo_lr_test + 0.3 * p_combo_xgb_test + 0.1 * p_combo_nbr_test\ntest[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)","metadata":{}},{"cell_type":"markdown","source":"from sklearn.model_selection import KFold\n\ndef kfoldTrainPredict(X, y, X_submission, clfs, n_folds):\n    np.random.seed(0)  # seed to shuffle the train set\n\n    kf = KFold(n_splits=2)\n    kf.get_n_splits(X)\n    kf = list(kf.split(X))\n    \n    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n    dataset_blend_validate = np.zeros((X_submission.shape[0], len(clfs)))\n\n    for j, clf in enumerate(clfs):\n        dataset_blend_validate_j = np.zeros((X_submission.shape[0], len(kf)))\n        for i, (train, validate) in enumerate(kf):\n            X_train = X[train]\n            y_train = y[train]\n            X_validate = X[validate]\n            y_validate = y[validate]\n            clf.fit(X_train, y_train)\n            y_submission = clf.predict(X_validate)\n            dataset_blend_train[validate, j] = y_submission\n            dataset_blend_validate_j[:, i] = clf.predict(X_submission)\n        dataset_blend_validate[:, j] = dataset_blend_validate_j.mean(1)\n\n\n    clf = LinearRegression()\n    clf.fit(dataset_blend_train, y)\n    y_submission = clf.predict(dataset_blend_validate)\n    return y_submission","metadata":{}},{"cell_type":"markdown","source":"clfs = [XGBRegressor(objective = \"reg:squarederror\", max_depth=10, n_estimators=100),\n       LinearRegression()]\nX = np.concatenate((X_stats, embeddings), 1)\nembeddingsTest = np.array([nlp(text).vector for text in test[\"excerpt\"]])\nX_stats_test = test[[\"wordCount\", \"wordLength\", \"sentenceLength\", \"sentenceCount\", \"syllables\"]]\n#X_submission = np.concatenate((X_stats_test, embeddingsTest), axis = 1)\n\n#y_submission = kfoldTrainPredict(X, y, X_submission, clfs, 10)","metadata":{}},{"cell_type":"markdown","source":"test[\"target\"] = y_submission\ntest[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-31T18:20:38.759897Z","iopub.status.idle":"2021-07-31T18:20:38.760627Z"}}},{"cell_type":"markdown","source":"#### Roberta","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n\n\n#optimizer = tf.keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.324575Z","iopub.status.idle":"2021-08-02T23:39:13.325379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from_saved = True\nkaggle = True\nhasInternet = False\n\nfrom transformers import RobertaTokenizer, TFRobertaModel\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\n\ntf.random.set_seed(1)\n\nclass MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, initial_learning_rate, learning_rate_scaling, epoch, batch_size, nsamples):\n        super().__init__()\n        self.total_step = tf.cast(nsamples/batch_size * epoch, tf.float32)\n        self.initial_learning_rate = tf.cast(initial_learning_rate, tf.float32)\n        self.learning_rate_scaling = tf.cast(learning_rate_scaling, tf.float32)\n\n    def __call__(self, step):\n        print(\"step: \", tf.cast(step, tf.float32), \", total: \", self.total_step)\n        #return self.initial_learning_rate / (step + 1)\n        r1 = (1 - self.learning_rate_scaling) * 10 * self.initial_learning_rate / self.total_step * step + self.learning_rate_scaling * self.initial_learning_rate\n        r2 = -self.initial_learning_rate / (0.9 * self.total_step) * step + 10/9 * self.initial_learning_rate\n        r = tf.cond(step < 0.1 * self.total_step, lambda: r1, lambda: r2)\n        print(\"r =\",r)\n\n        return r\n\ndef root_mean_squared_error(y_true, y_pred):\n    return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n\nif hasInternet:\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    roberta = TFRobertaModel.from_pretrained('roberta-base')\n    tokenizer.save_pretrained('tokenizer')\n    roberta.save_pretrained('roberta')\nelse:\n    tokenizer = RobertaTokenizer.from_pretrained('../input/roberta')\n    roberta = TFRobertaModel.from_pretrained('../input/roberta')\n\n#inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n#outputs = roberta(inputs)\n#last_hidden_states = outputs.last_hidden_state\n\ntf.random.set_seed(1)\nif kaggle:\n    train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n    test0 = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nelse:\n    train = pd.read_csv(\"train.csv\")\n    test0 = train[:7]\n\nn = train.shape[0]\nfrom sklearn.model_selection import train_test_split\ndf_train, df_valid = train_test_split(train, test_size = 0.15)\n\n\nepoch=50\nngpus = 1\nbatch_size = 16*ngpus\nMAX_LEN = 256\ndef encode_text(txt, labels, tokenizer):\n    for tt, ll in zip(txt, labels):\n        encoded = tokenizer.__call__(tt, max_length = MAX_LEN, add_special_tokens = True, truncation = True,\n                                     padding = 'max_length', return_attention_mask = True, return_tensors='tf')\n        #encoded = tokenizer.__call__(tt, max_length = MAX_LEN, return_tensors='tf', truncation=True)\n\n        yield (encoded.input_ids[0], encoded.attention_mask[0], ll)\nds = tf.data.Dataset.from_generator(lambda: encode_text(df_train.excerpt, df_train.target, tokenizer), \n                                    output_types = (tf.int32, tf.int32, tf.float64),\n                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))\nds = ds.map(lambda a,b,c: ({\"input_ids\":a, \"attention_mask\":b}, c)).batch(batch_size).repeat(epoch)\n\nds_valid = tf.data.Dataset.from_generator(lambda: encode_text(df_valid.excerpt, df_valid.target, tokenizer), \n                                    output_types = (tf.int32, tf.int32, tf.float64),\n                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))\nds_valid = ds_valid.map(lambda a,b,c: ({\"input_ids\":a, \"attention_mask\":b}, c)).batch(batch_size)\n\nn_test = test0.shape[0]; test_targets = np.array([1.0 for i in range(n_test)])\nds_test = tf.data.Dataset.from_generator(lambda: encode_text(test0.excerpt, test_targets, tokenizer), \n                                    output_types = (tf.int32, tf.int32, tf.float64),\n                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))\nds_test = ds_test.map(lambda a,b,c: ({\"input_ids\":a, \"attention_mask\":b}, c)).batch(batch_size)\n\nclass text_model(tf.keras.Model):\n    def __init__(self, base):\n        super(text_model,self).__init__(name=\"text_model\")\n        self.bert = base\n        #self.dense1 = tf.keras.layers.Dense(128, activation = 'relu', name = \"dense1\")\n        self.dense1 = tf.keras.layers.Dense(256, activation = 'relu', name = \"dense1\")\n        self.dense2 = tf.keras.layers.Dense(64, activation = 'relu', name = \"dense2\")\n        self.dense3 = tf.keras.layers.Dense(1, activation = 'linear', name = \"dense3\")\n    def call(self, x):\n        #print(\"pooler_output = \", self.bert(x).pooler_output.shape)\n        #print(\"dim = \", self.bert(x)[0][:,0,:].shape)\n        #print(\"last hidden = \", self.bert(x).last_hidden_state)\n        #yb = self.bert(x)[0][:,0,:]\n        yb = self.bert(x).pooler_output\n        yb = self.dense1(yb)\n        yb = self.dense2(yb)\n        yb = self.dense3(yb)\n        return yb\n\n\nif  not from_saved:\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        \"model.h5\",\n        monitor=\"val_loss\",\n        verbose=1,\n        save_best_only=True,\n        save_weights_only=True,\n        mode=\"auto\",\n        save_freq=\"epoch\",\n        options=None\n    )\n    n = df_train.shape[0]\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=5e-5, decay_steps=n/batch_size/ngpus, decay_rate=0.9)\n    #lr_schedule = MyLRSchedule(initial_learning_rate=3e-4, learning_rate_scaling=0.5, \n                                                             #epoch = epoch, batch_size = batch_size, nsamples=n)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\n    #optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5, epsilon = 1e-8)\n    #optimizer = tf.keras.optimizers.SGD(learning_rate = 5e-5)\n\n    loss_fn = root_mean_squared_error #tf.keras.losses.MeanSquaredError()\n    model = text_model(roberta)\n    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[root_mean_squared_error])\n    model.fit(ds, batch_size = batch_size, steps_per_epoch = n/batch_size/ngpus,\n              epochs = epoch, validation_data = ds_valid, callbacks=[checkpoint])\n    #model.save_weights(\"/tmp/model.weights\")\nelse:\n    model = text_model(roberta)\n    optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5, epsilon = 1e-8)\n    def root_mean_squared_error(y_true, y_pred):\n            return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n    loss_fn = root_mean_squared_error #tf.keras.losses.MeanSquaredError()\n    model = text_model(roberta)\n    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[root_mean_squared_error])\n    n = df_train.shape[0]\n    model.fit(ds, batch_size = batch_size, steps_per_epoch = 1, epochs = 1)\n\n#this is 128x64x0 model.load_weights(\"../input/model-weights-04934/model.h5\")  \nmodel.load_weights(\"../input/model-256-64-1-05134/model_256_64_1_05134.h5\")\np_roberta1 = model.predict(ds_test)[:,0]\nmodel.load_weights(\"../input/model-256-64-04983/model_256_64_1_04983.h5\")\np_roberta2 = model.predict(ds_test)[:,0]\n#print(\"rmse = \", root_mean_squared_error(p[:,0], df_valid.target))\n\n#p = model.predict(ds_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.326808Z","iopub.status.idle":"2021-08-02T23:39:13.327605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base 2\nfrom_saved = True\nkaggle = True\nhasInternet = False\n\nfrom transformers import RobertaTokenizer, TFRobertaModel\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\n\ntf.random.set_seed(1)\n\nclass MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, initial_learning_rate, learning_rate_scaling, epoch, batch_size, nsamples):\n        super().__init__()\n        self.total_step = tf.cast(nsamples/batch_size * epoch, tf.float32)\n        self.initial_learning_rate = tf.cast(initial_learning_rate, tf.float32)\n        self.learning_rate_scaling = tf.cast(learning_rate_scaling, tf.float32)\n\n    def __call__(self, step):\n        print(\"step: \", tf.cast(step, tf.float32), \", total: \", self.total_step)\n        #return self.initial_learning_rate / (step + 1)\n        r1 = (1 - self.learning_rate_scaling) * 10 * self.initial_learning_rate / self.total_step * step + self.learning_rate_scaling * self.initial_learning_rate\n        r2 = -self.initial_learning_rate / (0.9 * self.total_step) * step + 10/9 * self.initial_learning_rate\n        r = tf.cond(step < 0.1 * self.total_step, lambda: r1, lambda: r2)\n        print(\"r =\",r)\n\n        return r\n\ndef root_mean_squared_error(y_true, y_pred):\n    return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n\nif hasInternet:\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    roberta = TFRobertaModel.from_pretrained('roberta-base')\n    tokenizer.save_pretrained('tokenizer')\n    roberta.save_pretrained('roberta')\nelse:\n    tokenizer = RobertaTokenizer.from_pretrained('../input/roberta')\n    roberta = TFRobertaModel.from_pretrained('../input/roberta')\n\n#inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n#outputs = roberta(inputs)\n#last_hidden_states = outputs.last_hidden_state\n\ntf.random.set_seed(1)\nif kaggle:\n    train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n    test0 = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nelse:\n    train = pd.read_csv(\"train.csv\")\n    test0 = train[:7]\n\nn = train.shape[0]\nfrom sklearn.model_selection import train_test_split\ndf_train, df_valid = train_test_split(train, test_size = 0.15)\n\n\nepoch=50\nngpus = 1\nbatch_size = 16*ngpus\nMAX_LEN = 256\ndef encode_text(txt, labels, tokenizer):\n    for tt, ll in zip(txt, labels):\n        encoded = tokenizer.__call__(tt, max_length = MAX_LEN, add_special_tokens = True, truncation = True,\n                                     padding = 'max_length', return_attention_mask = True, return_tensors='tf')\n        #encoded = tokenizer.__call__(tt, max_length = MAX_LEN, return_tensors='tf', truncation=True)\n\n        yield (encoded.input_ids[0], encoded.attention_mask[0], ll)\nds = tf.data.Dataset.from_generator(lambda: encode_text(df_train.excerpt, df_train.target, tokenizer), \n                                    output_types = (tf.int32, tf.int32, tf.float64),\n                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))\nds = ds.map(lambda a,b,c: ({\"input_ids\":a, \"attention_mask\":b}, c)).batch(batch_size).repeat(epoch)\n\nds_valid = tf.data.Dataset.from_generator(lambda: encode_text(df_valid.excerpt, df_valid.target, tokenizer), \n                                    output_types = (tf.int32, tf.int32, tf.float64),\n                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))\nds_valid = ds_valid.map(lambda a,b,c: ({\"input_ids\":a, \"attention_mask\":b}, c)).batch(batch_size)\n\nn_test = test0.shape[0]; test_targets = np.array([1.0 for i in range(n_test)])\nds_test = tf.data.Dataset.from_generator(lambda: encode_text(test0.excerpt, test_targets, tokenizer), \n                                    output_types = (tf.int32, tf.int32, tf.float64),\n                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))\nds_test = ds_test.map(lambda a,b,c: ({\"input_ids\":a, \"attention_mask\":b}, c)).batch(batch_size)\n\nclass text_model(tf.keras.Model):\n    def __init__(self, base):\n        super(text_model,self).__init__(name=\"text_model\")\n        self.bert = base\n        #self.dense1 = tf.keras.layers.Dense(128, activation = 'relu', name = \"dense1\")\n        self.dense1 = tf.keras.layers.Dense(256, activation = 'relu', name = \"dense1\")\n        self.dense2 = tf.keras.layers.Dense(64, activation = 'relu', name = \"dense2\")\n        self.dense3 = tf.keras.layers.Dense(1, activation = 'linear', name = \"dense3\")\n    def call(self, x):\n        #print(\"pooler_output = \", self.bert(x).pooler_output.shape)\n        #print(\"dim = \", self.bert(x)[0][:,0,:].shape)\n        #print(\"last hidden = \", self.bert(x).last_hidden_state)\n        #yb = self.bert(x)[0][:,0,:]\n        yb = self.bert(x).pooler_output\n        yb = self.dense1(yb)\n        #yb = self.dense2(yb)\n        yb = self.dense3(yb)\n        return yb\n\n\nif  not from_saved:\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        \"model.h5\",\n        monitor=\"val_loss\",\n        verbose=1,\n        save_best_only=True,\n        save_weights_only=True,\n        mode=\"auto\",\n        save_freq=\"epoch\",\n        options=None\n    )\n    n = df_train.shape[0]\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=5e-5, decay_steps=n/batch_size/ngpus, decay_rate=0.9)\n    #lr_schedule = MyLRSchedule(initial_learning_rate=3e-4, learning_rate_scaling=0.5, \n                                                             #epoch = epoch, batch_size = batch_size, nsamples=n)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\n    #optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5, epsilon = 1e-8)\n    #optimizer = tf.keras.optimizers.SGD(learning_rate = 5e-5)\n\n    loss_fn = root_mean_squared_error #tf.keras.losses.MeanSquaredError()\n    model = text_model(roberta)\n    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[root_mean_squared_error])\n    model.fit(ds, batch_size = batch_size, steps_per_epoch = n/batch_size/ngpus,\n              epochs = epoch, validation_data = ds_valid, callbacks=[checkpoint])\n    #model.save_weights(\"/tmp/model.weights\")\nelse:\n    model = text_model(roberta)\n    optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5, epsilon = 1e-8)\n    def root_mean_squared_error(y_true, y_pred):\n            return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n    loss_fn = root_mean_squared_error #tf.keras.losses.MeanSquaredError()\n    model = text_model(roberta)\n    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[root_mean_squared_error])\n    n = df_train.shape[0]\n    model.fit(ds, batch_size = batch_size, steps_per_epoch = 1, epochs = 1)\n\n#this is 128x64x0 model.load_weights(\"../input/model-weights-04934/model.h5\")  \np_folds = [0,1,2,3,4]\nfor fold in range(0,5):\n    model.load_weights(\"../input/5fold-2561step5e5save10pass/model_fold\" + str(fold + 1) + \".h5\")\n    p = model.predict(ds_test)\n    p_folds[fold] = p[:,0]\n#print(\"rmse = \", root_mean_squared_error(p[:,0], df_valid.target))\n\n#p = model.predict(ds_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:40:57.287021Z","iopub.execute_input":"2021-08-02T23:40:57.287383Z","iopub.status.idle":"2021-08-02T23:41:46.844937Z","shell.execute_reply.started":"2021-08-02T23:40:57.287351Z","shell.execute_reply":"2021-08-02T23:41:46.843978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from_saved = True\nkaggle = True\nhasInternet = False\n\nfrom transformers import RobertaTokenizer, TFRobertaModel\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\n\ntf.random.set_seed(1)\n\ndef root_mean_squared_error(y_true, y_pred):\n    return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n\nif hasInternet:\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    roberta = TFRobertaModel.from_pretrained('roberta-base')\n    tokenizer.save_pretrained('tokenizer')\n    roberta.save_pretrained('roberta')\nelse:\n    tokenizer = RobertaTokenizer.from_pretrained('../input/tokenizerlarge')\n    roberta = TFRobertaModel.from_pretrained('../input/roberta-large')\n\n#inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n#outputs = roberta(inputs)\n#last_hidden_states = outputs.last_hidden_state\n\ntf.random.set_seed(1)\nif kaggle:\n    train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n    test0 = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nelse:\n    train = pd.read_csv(\"train.csv\")\n    test0 = train[:7]\n\nn = train.shape[0]\nfrom sklearn.model_selection import train_test_split\ndf_train, df_valid = train_test_split(train, test_size = 0.15)\n\n\nepoch=50\nngpus = 1\nbatch_size = 4*ngpus\nMAX_LEN = 256\ndef encode_text(txt, labels, tokenizer):\n    for tt, ll in zip(txt, labels):\n        encoded = tokenizer.__call__(tt, max_length = MAX_LEN, add_special_tokens = True, truncation = True,\n                                     padding = 'max_length', return_attention_mask = True, return_tensors='tf')\n        #encoded = tokenizer.__call__(tt, max_length = MAX_LEN, return_tensors='tf', truncation=True)\n\n        yield (encoded.input_ids[0], encoded.attention_mask[0], ll)\nds = tf.data.Dataset.from_generator(lambda: encode_text(df_train.excerpt, df_train.target, tokenizer), \n                                    output_types = (tf.int32, tf.int32, tf.float64),\n                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))\nds = ds.map(lambda a,b,c: ({\"input_ids\":a, \"attention_mask\":b}, c)).batch(batch_size).repeat(epoch)\n\nds_valid = tf.data.Dataset.from_generator(lambda: encode_text(df_valid.excerpt, df_valid.target, tokenizer), \n                                    output_types = (tf.int32, tf.int32, tf.float64),\n                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))\nds_valid = ds_valid.map(lambda a,b,c: ({\"input_ids\":a, \"attention_mask\":b}, c)).batch(batch_size)\n\nn_test = test0.shape[0]; test_targets = np.array([1.0 for i in range(n_test)])\nds_test = tf.data.Dataset.from_generator(lambda: encode_text(test0.excerpt, test_targets, tokenizer), \n                                    output_types = (tf.int32, tf.int32, tf.float64),\n                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))\nds_test = ds_test.map(lambda a,b,c: ({\"input_ids\":a, \"attention_mask\":b}, c)).batch(batch_size)\n\nclass text_model(tf.keras.Model):\n    def __init__(self, base):\n        super(text_model,self).__init__(name=\"text_model\")\n        self.bert = base\n        #self.dense1 = tf.keras.layers.Dense(128, activation = 'relu', name = \"dense1\")\n        self.dense1 = tf.keras.layers.Dense(256, activation = 'relu', name = \"dense1\")\n        self.dense2 = tf.keras.layers.Dense(64, activation = 'relu', name = \"dense2\")\n        self.dense3 = tf.keras.layers.Dense(1, activation = 'linear', name = \"dense3\")\n    def call(self, x):\n        #print(\"pooler_output = \", self.bert(x).pooler_output.shape)\n        #print(\"dim = \", self.bert(x)[0][:,0,:].shape)\n        #print(\"last hidden = \", self.bert(x).last_hidden_state)\n        #yb = self.bert(x)[0][:,0,:]\n        yb = self.bert(x).pooler_output\n        yb = self.dense1(yb)\n        yb = self.dense2(yb)\n        yb = self.dense3(yb)\n        return yb\n\n\nif  not from_saved:\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        \"model.h5\",\n        monitor=\"val_loss\",\n        verbose=1,\n        save_best_only=True,\n        save_weights_only=True,\n        mode=\"auto\",\n        save_freq=\"epoch\",\n        options=None\n    )\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=5e-5, decay_steps=n/batch_size/ngpus, decay_rate=0.9)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\n    #optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5, epsilon = 1e-8)\n    #optimizer = tf.keras.optimizers.SGD(learning_rate = 5e-5)\n\n    loss_fn = root_mean_squared_error #tf.keras.losses.MeanSquaredError()\n    model = text_model(roberta)\n    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[root_mean_squared_error])\n    n = df_train.shape[0]\n    model.fit(ds, batch_size = batch_size, steps_per_epoch = n/batch_size/ngpus,\n          epochs = epoch, validation_data = ds_valid, callbacks=[checkpoint])\n    #model.save_weights(\"/tmp/model.weights\")\nelse:\n    model = text_model(roberta)\n    optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5, epsilon = 1e-8)\n    def root_mean_squared_error(y_true, y_pred):\n            return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n    loss_fn = root_mean_squared_error #tf.keras.losses.MeanSquaredError()\n    model = text_model(roberta)\n    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[root_mean_squared_error])\n    n = df_train.shape[0]\n    model.fit(ds, batch_size = 1, steps_per_epoch = 1, epochs = 1)\n\n#this is 128x64x0 model.load_weights(\"../input/model-weights-04934/model.h5\")  \nmodel.load_weights(\"../input/model-large-256-64-1-048277/model_large_256_64_1_048277.h5\")\np_roberta3 = model.predict(ds_test)[:,0]\nmodel.load_weights(\"../input/model-large-256-64-1-046656/model_large_256_64_1_046656.h5\")\np_roberta4 = model.predict(ds_test)[:,0]\n#print(\"rmse = \", root_mean_squared_error(p[:,0], df_valid.target))\n\n#p = model.predict(ds_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.331641Z","iopub.status.idle":"2021-08-02T23:39:13.332454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_roberta = .3 * p_roberta4 + .3 * p_roberta3 + .3 * p_roberta2 + .1 * p_roberta1 \np_f = 0.2*p_folds[0] + 0.2*p_folds[1] + 0.2*p_folds[2] + 0.2*p_folds[3] + 0.2*p_folds[4]\np_roberta = .8 * p_roberta + .2 *p_f","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.333653Z","iopub.status.idle":"2021-08-02T23:39:13.33447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddingsTest = np.array([nlp(text).vector for text in test[\"excerpt\"]])\nX_stats_test = test[[\"wordCount\", \"wordLength\", \"sentenceLength\", \"sentenceCount\", \"syllables\"]]\np_combo_lr_test = model_combo_lr.predict(np.concatenate((X_stats_test, embeddingsTest), axis = 1))\np_combo_xgb_test = model_combo_xgb.predict(np.concatenate((X_stats_test, embeddingsTest), axis = 1))\ntest[\"target\"] = 0.03 * p_combo_lr_test + 0.02 * p_combo_xgb_test + .95 * p_roberta\ntest[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.335794Z","iopub.status.idle":"2021-08-02T23:39:13.336611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat submission.csv","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.33781Z","iopub.status.idle":"2021-08-02T23:39:13.338613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IDEAS TO TRY:\n# parts of speech (hypothesis: more adjectives/adverbs = easier readability)\n# Tfidf\n# word2vec\n","metadata":{"execution":{"iopub.status.busy":"2021-08-02T23:39:13.339837Z","iopub.status.idle":"2021-08-02T23:39:13.340759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}